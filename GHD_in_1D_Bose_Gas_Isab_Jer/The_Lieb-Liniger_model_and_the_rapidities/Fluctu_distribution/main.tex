\subsubsection{Rappel de Physique statistique}

On écrit l'observable énergie et nombre :
	\begin{eqnarray}
		\operator{\mathcal{N}} & = & \sum_{\vert \{\theta_a\} \rangle }   \left ( \sum_{a = 1}^N  1 \right )  \vert \{ \theta_a\}\rangle	\langle \{ \theta_a \} \vert,\\
		\operator{\mathcal{E}} & = & \sum_{ \vert \{ \theta_a\} \rangle }  \left ( \sum_{a = 1}^N  \varepsilon ( \theta_a ) \right )   \vert \{ \theta_a\}\rangle	\langle \{ \theta_a \}\vert,		
	\end{eqnarray}
	
	avec $\sum_{a = 1}^N 1 \equiv \langle \operator{\mathcal{N}} \rangle_{\vert \{\theta_a \} \rangle}  \doteq  \langle \{ \theta_a \} \vert  \operator{\mathcal{N}}  \vert \{ \theta_a\}\rangle  $ et $  \sum_{a = 1}^N  \varepsilon ( \theta_a ) \equiv \langle \operator{\mathcal{E}} \rangle_{\vert \{\theta_a \} \rangle}  \doteq  \langle \{ \theta_a \} \vert  \operator{ \mathcal{E}}  \vert \{ \theta_a\}\rangle $.

	
	La probabilité que le système soit dans configuration $\vert \{ \theta_a \}\rangle $  est 
	\begin{eqnarray}
		P_{\vert \{\theta_a \} \rangle} & = & \frac{e^{- \beta \left ( \langle \operator{\mathcal{E}} \rangle_{\vert \{\theta_a \} \rangle}   - \mu \langle \operator{\mathcal{N}} \rangle_{\vert \{\theta_a \} \rangle} \right )}}{Z_{thermal}} = \frac{e^{- \beta \sum_{a=1}^N  ( \varepsilon( \theta_a )   - \mu  )}}{Z_{thermal}}	
	\end{eqnarray}
	
	avec la fonction de partition $Z_{thermal} = \sum_{ \vert \{ \theta_a\} \rangle }e^{- \beta \left ( \langle \operator{\mathcal{E}} \rangle_{\vert \{\theta_a \} \rangle}   - \mu \langle \operator{\mathcal{N}} \rangle_{\vert \{\theta_a \} \rangle} \right )} = \sum_{ \vert \{ \theta_a\} \rangle } e^{- \beta \sum_{a=1}^N  ( \varepsilon( \theta_a )   - \mu  )}$
	
	On peut commence à généraliser avec l'operateur :
	
	
	\begin{eqnarray}
		\operator{\mathcal{O}}_i & = & \sum_{\vert \{\theta_a \} \rangle} \langle \operator{\mathcal{O}}_i \rangle_{\vert \{\theta_a \} \rangle}  \vert \{ \theta_a\}\rangle	\langle \{ \theta_a \} \vert
	\end{eqnarray}
	
	$\operator{\mathcal{O}}_i \in \{\operator{\mathcal{N}} , \operator{\mathcal{E}} - \mu \operator{\mathcal{N}} \} $  tel que $\sum_i \beta_i \langle \operator{\mathcal{O}}_i \rangle_{\vert \{\theta_a \} \rangle} = \beta \left ( \langle \operator{\mathcal{E}} \rangle_{\vert \{\theta_a \} \rangle}   - \mu \langle \operator{\mathcal{N}} \rangle_{\vert \{\theta_a \} \rangle} \right ) $ et pour simplifier ici $Z \equiv Z_{thermal}$:
	
	Sa  moyenne , variance et équatype de l'observable :
		
	\begin{eqnarray}
		\overline{\operator{\mathcal{O}}_i } & = & \sum_{\vert \{\theta_a \} \rangle} 	\langle \operator{\mathcal{O}}_i \rangle_{\vert \{\theta_a \} \rangle} \overbrace{\frac{e^{-\sum_i \beta_i \langle \operator{\mathcal{O}}_i \rangle_{\vert \{\theta_a \} \rangle} }}{Z}}^{P_{\vert \{\theta_a \} \rangle}} = - \frac{1}{Z}\frac{ \partial Z }{ \partial \beta_i } =  - \frac{ \partial \ln Z }{ \partial \beta_i } \\
		\overline{\operator{\mathcal{O}}_i^2 } & = & \sum_{\vert \{\theta_a \} \rangle} 	\langle \operator{\mathcal{O}}_i \rangle_{\vert \{\theta_a \} \rangle}^2 \frac{e^{-\sum_i \beta_i \langle \operator{\mathcal{O}}_i \rangle_{\vert \{\theta_a \} \rangle} }}{Z} = \frac{1}{Z} \frac{ \partial^2 Z }{ {\partial \beta_i}^2 }\\
		\Delta_{\operator{\mathcal{O}}_i}^2  & = & 	\overline{(\operator{\mathcal{O}}_i - \overline{\operator{\mathcal{O}}_i})^2 }  = 	\overline{\operator{\mathcal{O}}_i^2 }  -  \overline{\operator{\mathcal{O}}_i }^2 = \frac{1}{Z} \frac{ \partial^2 Z }{ {\partial \beta_i}^2 } - \left ( \frac{1}{Z}\frac{ \partial Z }{ \partial \beta_i }\right )^2 = \frac{\partial}{\partial \beta_i } \left ( \frac{1}{Z} \frac{\partial Z}{\partial \beta_i } \right )	
	\end{eqnarray}
	
	si $\operator{\mathcal{O}}_i = \operator{\mathcal{N}}$ alors $\beta_i = - \beta \mu $ et si $\operator{\mathcal{O}}_i = \operator{\mathcal{E}} - \mu \operator{\mathcal{N}} $ alors $\beta_i = \beta$.\\
	
	\begin{eqnarray}
		\overline{\operator{\mathcal{O}}_i } & = & 	\sum_{\vert \{\theta_a \} \rangle} 	\langle \{\theta_a \}  \vert \operator{\mathcal{O}}_i \vert \{\theta_a \}  \rangle \frac{e^{-\sum_i \beta_i \langle \operator{\mathcal{O}}_i \rangle_{\vert \{\theta_a \} \rangle} }}{Z},\\
		& = & 	\sum_{\vert \{\theta_b \} \rangle} \langle \{\theta_b \}  \vert  \operator{\mathcal{O}}_i \sum_{\vert \{\theta_a \} \rangle }\frac{e^{-\sum_i \beta_i \langle \operator{\mathcal{O}}_i \rangle_{\vert \{\theta_a \} \rangle} }}{Z} \vert \{\theta_a \}  \rangle  	\langle \{\theta_a \}  \vert  \{\theta_b \}  \rangle ,\\
		& = & Tr (  \operator{\mathcal{O}}_i \operator{\rho}) 
	\end{eqnarray}
	
	avec $\operator{\rho} = \sum_{\vert \{\theta_a \} \rangle }\frac{e^{-\sum_i \beta_i \langle \operator{\mathcal{O}}_i \rangle_{\{\theta_a \}} }}{Z} \vert \{\theta_a \}  \rangle   	\langle \{\theta_a \} \vert $ et $Z = \sum_{\vert \{\theta_a \} \rangle }e^{-\sum_i \beta_i \langle \operator{\mathcal{O}}_i \rangle_{ \{\theta_a \} } } $ tel que $Tr (  \operator{\rho}) = 1 $\\
	
	La matrice densité thermique est :
	\begin{eqnarray}
		\operator{\rho}_{thermal} & = & \frac{e^{- \beta \operator{H}}}{Z_{thermal}}, \\
		e^{-\beta \operator{H}} & = & 	\sum_{\vert \theta_a \rangle} e^{- \beta \sum_{a=1}^N ( \varepsilon(\theta_a)- \mu ) } \vert \{ \theta_a\} \rangle \langle  \{ \theta_a\}  \vert 
	\end{eqnarray}


La matrice densité GGE avec $Z \equiv Z_{GGE}$ est :
	\begin{eqnarray}
		\operator{\rho}_{GGE}[f] & = & \sum_{\vert \{\theta_a \} \rangle }\frac{e^{-\sum_{i = 1}^\infty  \beta_i \langle \operator{\mathcal{O}}_i \rangle_{\vert \{\theta_a \} \rangle} }}{Z} \vert \{\theta_a \}  \rangle   	\langle \{\theta_a \} \vert. 
	\end{eqnarray}
	
	Dans le cas thermique, on peut remarquer que $\langle \operator{\mathcal{N}} \rangle_{\{ \theta_a\} } \propto \sum_{a = 1}^N \theta_a^0 $ et $\langle \operator{\mathcal{E}} \rangle_{\{ \theta_a\} } \propto \sum_{a = 1}^N \theta_a^2 $. On peut donc réécrire $\sum_{i = 1}^\infty  \beta_i \langle \operator{\mathcal{O}}_i \rangle_{ \{\theta_a \} }$
	
	\begin{eqnarray}
		\sum_{i = 1}^\infty  \beta_i \langle \operator{\mathcal{O}}_i \rangle_{ \{\theta_a \} } & = & \sum_{i = 0}^\infty \alpha_i \sum_{a = 1 }^N \theta_a^i		
	\end{eqnarray}
	
	et pour chaque $a \in \llbracket 1 , N  \rrbracket \colon \sum_i \alpha_i \theta_a^i$ converge donc on peut échanger les deux sommes soit 
	
	\begin{eqnarray}
		\sum_{i = 1}^\infty  \beta_i \langle \operator{\mathcal{O}}_i \rangle_{ \{\theta_a \} } & = & \sum_{a = 1 }^N  f(\theta_a) 
	\end{eqnarray}
	
	avec $f(\theta) =  \sum_{i = 0}^\infty \alpha_i  \theta^i$.	 Et on peut réecrire la matrice densité  :
	
	\begin{eqnarray}
		\operator{\rho}_{GGE}[f] & = & \frac{e^{-\operator{Q}[f]}}{Z_{GGE}}, \\
		e^{-\operator{Q}[f]} & = & 	\sum_{\vert \{\theta_a \} \rangle } e^{- \sum_{a = 1}^N f(\theta_a) } \vert \{ \theta_a\} \rangle \langle  \{ \theta_a\}  \vert 
	\end{eqnarray}
 
	
	pour une certaine fonction $f$ relié à la charge $\operator{Q} [f]  = \sum_{\vert \{\theta_a \} \rangle } \left ( \sum_{a = 1}^N f ( \theta_a )  \right ) \vert \{ \theta_a \} \rangle \langle \{ \theta_a \} \vert $.
	Et on peut réecrire la probabilité de la configuration $\{\theta_a\}$ : $ P_{\{ \theta_a \}} = \langle \{ \theta_a \}\vert \operator{\rho}_{GGE}[f] \vert  \{ \theta_a \} \rangle = e^{-\sum_{a = 1}^N f(\theta_a)} / Z $ avec $Z = \sum_{\vert \{\theta_a \} \rangle } e^{-\sum_{a = 1}^N f(\theta_a)}$.\\
	
	 Nous aimerions calculer les valeurs d'attente par rapport à cette matrice de densité, par exemple
	La moyenne GGE d'un observable s'écrit ,
	
	\begin{eqnarray}
		\langle \operator{\mathcal{O}} \rangle_{GGE} & \doteq & \displaystyle  \frac{\text{Tr} (\operator{\mathcal{O}}\operator{\rho}_{GGE}[f])}{\text{Tr} (\operator{\rho}_{GGE}[f])} = \frac{\text{Tr} (\operator{\mathcal{O}}e^{-\operator{Q}[f]})}{\text{Tr} (e^{-\operator{Q}[f]})}	 = \frac{ \sum_{\vert \{\theta_a \}\rangle} \langle  \{ \theta_a\}  \vert   \operator{\mathcal{O}} \vert \{ \theta_a\} \rangle e^{- \sum_{a = 1}^N f(\theta_a) }  }{\sum_{\vert  \{\theta_a  \} \rangle} e^{- \sum_{a = 1}^N  f(\theta_a) } }
		%& =  & \frac{ \sum_{\pi} \sum_{\vert \{\theta_a \}\rangle \vert \Pi } \langle  \{ \theta_a\}  \vert   \operator{\mathcal{O}} \vert \{ \theta_a\} \rangle e^{- \sum_{a = 1}^N f(\theta_a) }  }{\sum_{\pi} \sum_{\vert \{\theta_a \}\rangle \vert \Pi }  e^{- \sum_{a = 1}^N  f(\theta_a) } }
	\end{eqnarray}
		
	pour une certaine observable $\operator{\mathcal{O}}$.\\

\subsubsection{.....}
	
	Dans la sous-sous-section précédente, nous avons expliqué comment les observables physiques, telles que les valeurs d'attente des charges et des courants, deviennent des fonctionnelles de la distribution de rapidité extensive $\Pi(\theta)$ dans la limite thermodynamique. Cependant, nous n'avons pas expliqué comment construire des distributions de rapidité physiquement significatives (sauf pour l'état fondamental de l'hamiltonien de Lieb-Liniger, pour lequel $\nu(\theta)$) est une fonction rectangulaire, voir la sous-section (??)). {\em Par exemple, quelle est la distribution de rapidité correspondant à un état d'équilibre thermique à une température non nulle ?}\\
	
	La question a été répondue dans les travaux pionniers de Yang et Yang (1969), que nous allons maintenant examiner brièvement. Tout d'abord, nous observons qu'il existe de nombreuses choix différents de séquences d'états propres $(\{\theta_a\}_{ a \in \llbracket 1 , N \rrbracket} )_{ N \in \mathbb{Z}}$ qui conduisent à la même distribution de rapidité thermodynamique (??). La description du système en termes de distribution de rapidité extensive $\Pi( \theta ) $ est seulement une description grossière : on devrait considérer la distribution de rapidité $\Pi ( \theta ) $ comme caractérisant un macro-état du système, correspondant à un très grand nombre de micro-états possibles $\vert \{ \theta_a \} \rangle $. {\em Pour faire de la thermodynamique, il faut estimer le nombre de ces micro-états.}\\
	
	Pour estimer ce nombre, on se concentre sur une petite cellule de rapidité $[\theta, \theta+\delta\theta]$, qui contient $\Pi(\theta) \delta \theta$ rapidités. 
	
	\begin{figure}[H]
		\centering 
		\begin{tikzpicture}
			%\input{The_Lieb-Liniger_model_and_the_rapidities/Fluctu_distribution/figures/Occupation_code}	
			\begin{scope}[transform canvas={scale=0.5}]
			\input{The_Lieb-Liniger_model_and_the_rapidities/Fluctu_distribution/figures/Occupation_theta_code}	
			\end{scope}
			
			\draw[color = red , scale = 0.5] (-13.5 , -1) rectangle (13 , 10) ; 
				
			
		\end{tikzpicture}	
		\captionsetup{skip=10pt} % Ajoute de l’espace après la légende
	\end{figure}
	
	
	
	
Les équations de Bethe (??) relient ces rapidités aux moments de fermions pa dans une cellule de moment $[p, p+\delta p]$, où $\delta \Pi/\delta \theta$ est d'environ $2\pi \Pi_s(\theta)$, voir l'équation (??). Il est important de noter que les moments de fermions $p_a$ sont soumis au principe d'exclusion de Pauli. Le nombre de micro-états est alors évalué en comptant le nombre de configurations de moments de fermions mutuellement distincts, équivalent à $\Pi (\theta)\delta \theta$, qui peuvent être placées dans la boîte $[p, p + \delta p]$. Comme l'espacement minimal entre deux moments est de $2\pi /L$, la réponse est :
	
	\begin{eqnarray}
		\# \mbox{conf.}(\theta) & \approx  & \frac{[ \Pi_s ( \theta ) \delta \theta ] ! }{ [ \Pi ( \theta ) \delta \theta ] ! [( \Pi_s ( \theta ) - \Pi ( \theta ) )  \delta \theta ] ! } , 	
	\end{eqnarray}
	
	or avec la formule de Sterling :  
	\begin{eqnarray}
		n! & \underset{n \to \infty}{\sim} n^n e^{-n} \sqrt{2\pi n}.,
	\end{eqnarray}
	
	composé du fonction logarithmique, il vient cette équivalence : 
	\begin{eqnarray}
		\ln n! & \underset{n \to \infty}{\rightarrow} & n \ln n \underbrace{- n + \ln \sqrt{2 \pi n }}_{o \left ( n \ln n \right ) } ,\\
		&  \underset{n \to \infty}{\sim} & n \ln n  
	\end{eqnarray}
	
	$\# \mbox{conf.}$ est jamais null donc on peut écrire que : 
\begin{eqnarray}
    \ln \# \mbox{conf.}(\theta) & \underset{\underset{\Pi (\theta )\leq  \Pi_s (\theta )}{\Pi \delta \theta  \to \infty}}{\sim}   & [ \Pi_s\ln \Pi_s - \Pi \ln \Pi - ( \Pi_s - \Pi ) \ln ( \Pi_s - \Pi) ] (\theta )\delta \theta .
\end{eqnarray}

Le nombre total de micro-états est le produit de toutes ces configurations pour toutes les cellules de rapidité $[\theta, \theta + \delta \theta]$. En prenant le logarithme et en remplaçant la somme par une intégrale sur $d \theta$, nous obtenons l'entropie de Yang-Yang :
\begin{eqnarray}
    \ln \# \mbox{microstates.} & = & \sum_{a\vert tranche} \ln \# \mbox{conf.}(\theta_a), \\
    & \approx &   S_{YY} [ \Pi ] , 	
\end{eqnarray}

\begin{eqnarray}
    S_{YY}[\Pi] & \doteq & \sum_{a\vert tranche} \, [ \Pi_s\ln \Pi_s - \Pi \ln \Pi - ( \Pi_s - \Pi ) \ln ( \Pi_s - \Pi ) ] (\theta_a) \delta \theta .
\end{eqnarray}
	
	Les variation de $f$ sont négligeables sur $\delta \theta $ alors  $\sum_{a = 1}^N  f(\theta_a) = \sum_{a \vert tranche } f(\theta_a) \Pi( \theta_a)\delta \theta$.
	
	
	\begin{eqnarray}
		\langle \operator{\mathcal{O}} \rangle_{GGE} & =  & \frac{  \sum_{\vert \{\theta_a \}\rangle \vert \Pi } \langle  \{ \theta_a\}  \vert   \operator{\mathcal{O}} \vert \{ \theta_a\} \rangle \# \mbox{microstates.} e^{- \sum_{a \vert tranche} f(\theta_a) \Pi ( \theta_a )  \delta \theta}    }{ \sum_{\vert \{\theta_a \}\rangle \vert \Pi }  \# \mbox{microstates.}e^{- \sum_{a \vert tranche }  f(\theta_a) \Pi ( \theta_a ) \delta \theta } }
	\end{eqnarray}
	
	Lorsque l'observable $\operator{\mathcal{O}}$ est suffisamment local, on croit que la valeur d'attente $\langle  \{ \theta_a\}  \vert   \mathcal{O} \vert \{ \theta_a\} \rangle$ ne dépend pas de l'état microscopique spécifique du système, de sorte qu'elle devient une fonctionnelle de $\Pi$ dans la limite thermodynamique.
	\begin{eqnarray}
		\underset{\mbox{\tiny therm.}}{\lim} \langle  \{ \theta_a\}  \vert   \operator{\mathcal{O}} \vert \{ \theta_a\} \rangle & = & \langle \operator{\mathcal{O}}\rangle_{[\Pi]},
	\end{eqnarray}
	
	et 
	
	\begin{eqnarray}
		\underset{\mbox{\tiny therm.}}{\lim} \langle \operator{\mathcal{O}} \rangle_{GGE} & =  & \frac{ \sum_{\Pi}  \langle \operator{\mathcal{O}}\rangle_{[\Pi]} \# \mbox{microstates.} e^{- \sum_{a \vert tranche} f(\theta_a) \Pi ( \theta_a )  \delta \theta}    }{\sum_{\Pi} \# \mbox{microstates.}  e^{- \sum_{a \vert tranche }  f(\theta_a) \Pi ( \theta_a ) \delta \theta } },\\
		& = & \frac{ \sum_{\Pi}  \langle \operator{\mathcal{O}}\rangle_{[\Pi]}  e^{ S_{YY}[\Pi]- \sum_{a \vert tranche} f(\theta_a) \Pi ( \theta_a )  \delta \theta}    }{\sum_{\Pi}  e^{S_{YY}[\Pi] - \sum_{a \vert tranche }  f(\theta_a) \Pi ( \theta_a ) \delta \theta } }
	\end{eqnarray}
	
\subsubsection{.....}

	Maintenant $\{\theta_a \}$ désigne l'ensemble de rapidité où $a$ est l'indice des tranches. La probalilité de cette configuration $\{ \pi^d(\theta_a) \}$ est : 
	
	\begin{eqnarray}
		P_{\{ \pi^d(\theta_a)\} } & = & \frac{ e^{ A[\pi^d]  }}{Z} 	
	\end{eqnarray}
	
	ou $A[\pi^d] = S_{YY}[\pi^d] - \sum_{a\vert tranche} f(\theta_a) \pi^d ( \theta) \delta \theta $. On fait l'hypothèse que la configuration $\{ \overline{\pi}_a^d \}$ domine tous les configurations (ie $\forall \{ \theta_a\}  \colon P_{\{ \pi^d(\theta_a)\} } \leq P_{\{ \overline{\pi}^d(\theta_a)\} }$) ce qui implique que  
	\begin{eqnarray*}
		\forall \theta_a  \in \{\theta_a \}\colon  \left ( \frac{\partial A}{\partial \pi^d(\theta_a) } [\overline{\pi}^d]	 = 0 , ~ \& \,  \forall \theta_b \in \{\theta_a \} \colon  \forall \pi^d \colon \frac{\partial^2 A}{\partial \pi^d(\theta_a) \partial \pi^d(\theta_b) } [\overline{\pi^d}] \leq  0 	\right ) 
	\end{eqnarray*}
	
	\begin{figure}[H]
		\centering 
		\begin{tikzpicture}
			\begin{scope}[transform canvas={scale=0.5}]
			\input{The_Lieb-Liniger_model_and_the_rapidities/Fluctu_distribution/figures/fonc_A_code}	
			
			\end{scope}
			
			\draw[color = red , scale = 0.5] (-2 , -1) rectangle (5, 6) ; 
				
			
		\end{tikzpicture}	
		\captionsetup{skip=10pt} % Ajoute de l’espace après la légende
	\end{figure}
	
	On note $\pi^d = \overline{\pi}^d + \delta \pi^d $, donc le debellopement limité de A autour de $\overline{\pi}^d$ est :
	\begin{eqnarray}
		A[\pi^d]  &  =  & 	A[\overline{\pi}^d]  + \frac{1}{1!} \sum_{a \vert tranche } \left \{ A^{(1)}\right \}_{a} [\overline{\pi}^d] \delta \pi^d(\theta_a)  + \frac{1}{2!} \sum_{a , b \vert tranche } \left \{ A^{(2)}\right \}_{a, b } [\overline{\pi}^d] \delta \pi^d(\theta_a)\delta  \pi^d(\theta_b) + R[\overline{\pi}^d], 
	\end{eqnarray}
	
	avec $ \left \{ A^{(k)}\right \}_{ i_1 , \cdots ,  i_k  }  = \frac{\partial^k A }{ \partial \pi^d ( \theta_{i_1} ) \cdots \partial \pi^d ( \theta_{i_k} )  } $ et $R[\overline{\pi}^d] = \sum_{ k = 3 }^\infty \frac{1}{k!} \sum_{i_1 , \cdots , i_k } \left \{ A^{(k)}\right \}_{ i_1 , \cdots ,  i_k  } [ \overline{\pi}^d ] \delta \pi^d(\theta_{i_1}) \cdots   \delta \pi^d(\theta_{i_k})$.
	
	On se restrins au second ordre soit $e^{R[\overline{\pi}^d]} \sim 1 $ soit 
	
	\begin{eqnarray}
		P_{\{\pi^d(\theta_a)\} } & 	\sim & B e^{\displaystyle \frac{1}{2} \sum_{a , b \vert tranche } \left \{ A^{(2)}\right \}_{a, b } [\overline{\pi}^d] \delta \pi^d(\theta_a)\delta  \pi^d(\theta_b) }, 
	\end{eqnarray}
	
	avec $ B = e^{A[\overline{\pi}^d]}/Z $.
	
	On reconnais une loi normale multidimensionelle
	\footnote{
	{\em \bf Loi normale multidimensionnelle} \\
	\begin{itemize}
		\item $x \in \mathbb{R}^N$ : fonction caractéristique : $\phi_{\mu , \Sigma} = \exp \left ( i x^T \mu - \frac{1}2 x^T \Sigma x  \right ) $.
		\item cas non-dégénéré où $Sigma$ est définie positive : $f_{ \mu , \Sigma } (x) = \frac{1}{(2 \pi)^{N/2} \det ( \Sigma ) ^{1/2}} \exp \left ( - \frac{1}{2} ( x - \mu )^T \Sigma^{-1} ( x - \mu ) \right ) $
		\item théorème centrale limite fait apparaitre un variable U de Gauss centré réduite 
			$$
			\begin{array}{cccc}
				& \mathbb{E} (U) = 0, & \mathbb{E} (U^2) = 1 & p_U(u) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{1}{2} u^2 } \\
				\mbox{ si $X = \sigma U + \mu $}, & \mathbb{E} (X) = \mu, & \mathbb{E} ((X-\mu)^2) = \sigma^2 & p_X(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x - \mu)^2}{2 \sigma^2}  }
			\end{array}
			$$
		\item loi unitaire à plusieurs variable 
			$$
			\begin{array}{ccc}
				 \mathbb{E} (U) = 0, & \mathbb{E} (UU^T) = id  & p_U(u) = \frac{1}{(2 \pi)^{N/2} } e^{-\frac{1}{2} u u^T } \\
			\end{array}
			$$
		\item loi générale à plusieurs variables
			$$
			\begin{array}{cccc}
				 X = a U + \mu & \mathbb{E} (X) = a \mathbb{E}(U) + \mu = \mu , & \mathbb{E} ((X-\mu)(X-\mu)^T) = \mathbb{E} (aUU^T a^T) = a a^T = \Sigma & p_X(x) = \frac{1}{(2 \pi)^{N/2}  \vert \Sigma\vert^2 } e^{-\frac{1}{2} (x-\mu)^T \Sigma^{-1} (x-\mu) } \\
			\end{array}
			$$
	\end{itemize}
	
	}
	:
	
	\begin{eqnarray}
		P_{\{\pi^d(\theta_a)\} } & \sim & 	\frac{1}{(2 \pi)^{N/2} \det ( \Sigma ) ^{1/2}} \exp \left ( - \frac{1}{2} ( x - \mu )^T \Sigma^{-1} ( x - \mu ) \right )	,
	\end{eqnarray}
	avec $( x - \mu ) = (\delta \pi^d ( \theta_1) , \underset{tranche}{ \cdots} )$ , $Z \sim (2 \pi)^{N/2} A[\overline{\pi}^d] \det ( \Sigma )^{1/2}$ et la matrice écartype :
	\begin{eqnarray}
		\Sigma & =& - \left [ A^{(2)} \right ]^{-1} 	
	\end{eqnarray}



\subsubsection{.....}
	

	
	\begin{eqnarray}
			\frac{ \delta S_{YY} }{ \delta \Pi(\theta_b)  } [\Pi ]  & = & \sum_{a\vert tranche}   \delta \theta \left \{ \ln \left ( \frac{ \Pi_s}{ \Pi  } - 1 \right )\frac{\delta \Pi  }{ \delta \Pi(\theta_b)} -  \left ( \ln \left ( 1 - \frac{ \Pi}{ \Pi_s }\right ) \right )\frac{\delta \Pi_s  }{ \delta \Pi(\theta_b) } \right \} (\theta_a) ,\\
			\frac{ \delta^2 S_{YY} }{ \delta \Pi(\theta_c) \delta \Pi(\theta_b)  } [\Pi ]  & = & 	\sum_{a \vert tranche}   \delta \theta \left \{ \left ( \Pi  \frac{\delta \Pi_s  }{\delta \Pi ( \theta_c) }- \Pi_s  \frac{\delta \Pi  }{\delta \Pi ( \theta_c ) } \right ) \right.   \\
			&&   \left. \left ( \frac{1}{\Pi_s\Pi  -\Pi^2  }\frac{\delta \Pi }{\delta \Pi(\theta_b) }- \frac{1}{\Pi_s^2  - \Pi \Pi_s }\frac{\delta \Pi_s }{\delta \Pi(\theta_b) } \right )  \right \}(\theta_a).
		\end{eqnarray}
		
	Or 
	\begin{eqnarray}
		2\pi \Pi_s  & = &  L + ( \Delta \star \Pi ) \delta \theta, 		
	\end{eqnarray}
	
	avec le produit de convolution discret $( f \star g )(x) = \sum_{x'\vert tranche } f( x - x') g(x') $, donc 
	\begin{eqnarray}
			\frac{\delta \Pi_s }{\delta \Pi( \theta_b)}   & = &\left  ( \frac{\Delta}{2\pi} \star \frac{\delta \Pi}{\delta \Pi(\theta_b) }  \right )	
		\end{eqnarray}
	
	\begin{eqnarray}
			\frac{ \delta S_{YY} }{ \delta \Pi(\theta_b)} [\Pi ]  & = & \sum_{a\vert tranche}   \delta \theta \left \{ \ln \left ( \frac{ \Pi_s }{ \Pi} - 1 \right )\frac{\delta \Pi }{ \delta \Pi (\theta_b) } -  \left ( \ln \left ( 1 - \frac{ \Pi}{ \Pi_s}\right ) \right ) \left ( \frac{ \Delta}{2 \pi} \star \frac{\delta \Pi }{ \delta \Pi(\theta_b) }  \right)  \delta \theta \right \} ( \theta_	a ),\\
			\frac{ \delta^2 S_{YY} }{ \delta \Pi(\theta_c) \delta \Pi(\theta_b)  } [\Pi ]  & = & \sum_{a \vert tranche}   \delta \theta \left \{  \frac{\Pi_s}{(\Pi^2  - \Pi_s\Pi)} \frac{(\delta \Pi)^2}{ \delta \Pi(\theta_c) \delta \Pi(\theta_b)  } \right . \\
		&  &  +  \left . \frac{1 }{(\Pi_s -\Pi) } \left [ \left (  \frac{\Delta}{2\pi} \star\frac{\delta \Pi}{\delta \Pi(\theta_c)}   \right )   \delta \theta \frac{\delta \Pi}{\delta \Pi(\theta_b)}+ \frac{\delta \Pi}{\delta \Pi(\theta_c)} \left (  \frac{\Delta}{2\pi} \star\frac{\delta \Pi}{\delta \Pi(\theta_b)} \right )  \delta \theta  \right ] \right . \\
		& & + \left .  \frac{\Pi }{\Pi_s\Pi - \Pi_s^2 }\left (  \frac{\Delta}{2\pi} \star\frac{\delta \Pi}{\delta \Pi(\theta_c)} \right )\left (  \frac{\Delta}{2\pi} \star\frac{\delta \Pi}{\delta \Pi(\theta_b)} \right )   (\delta \theta)^2 \right \}(\theta_a),	
	\end{eqnarray}
	
%	or 
%	
%		\begin{eqnarray}
%			\left  ( \frac{\Delta}{2\pi} \star \frac{\delta \Pi}{\delta \Pi(\theta_b) }  \right ) & = & 	\left  ( \frac{\Delta}{2\pi} \star  1  \right ) \frac{\delta \Pi}{\delta \Pi(\theta_b)}
%		\end{eqnarray}
		
		En remarquant que $\Delta$ est symetrique il vient que : 
	\begin{eqnarray}
		\sum_{a\vert tranche}   \,  \left \{ g  \cdot  	\left  ( \frac{\Delta}{2\pi} \star  \frac{\delta \Pi}{\delta \Pi(\theta_b)} \right ) \right \}( \theta_a) & = & \sum_{a\vert tranche}   \,  \left \{ g  \cdot  \sum_{c\vert tranche}    	
		 \frac{\Delta ( \cdot - \theta_c)}{2\pi}  \frac{\delta \Pi (\theta_c) }{\delta \Pi(\theta_b)}\right \}( \theta_a),\\
		 & = & \sum_{c\vert tranche}   \,  \left \{  \sum_{a\vert tranche}      	
		  \left ( \frac{\Delta ( \theta_a - \cdot )}{2\pi}  g ( \theta_a) \right ) \frac{\delta \Pi (\cdot) }{\delta \Pi(\theta_b)}\right \}( \theta_c),\\
		  & = & \sum_{c\vert tranche}  \,  \left \{  \sum_{a\vert tranche}      	
		  \left ( \frac{\Delta ( \cdot - \theta_a)}{2\pi}  g ( \theta_a) \right ) \frac{\delta \Pi (\cdot) }{\delta \Pi(\theta_b)}\right \}( \theta_c),\\
		  & = & \sum_{a\vert tranche}   \,  \left \{        	
		  \left ( \frac{\Delta}{2\pi}   \star g \right ) \frac{\delta \Pi  }{\delta \Pi(\theta_b)}\right \}( \theta_a),\\
	\end{eqnarray}	
	
	\begin{eqnarray}
			\frac{ \delta S_{YY} }{ \delta \Pi(\theta_b)} [\Pi ]  & = & \sum_{a\vert tranche}   \delta \theta \left \{ \left [ \ln \left ( \frac{ \Pi_s }{ \Pi} - 1 \right ) -  \left ( \frac{ \Delta}{2 \pi} \star \ln \left ( 1 - \frac{ \Pi}{ \Pi_s}\right )  \right)  \delta \theta\right ]\frac{\delta \Pi }{ \delta \Pi (\theta_b) } \right \} ( \theta_	a ),\\
			\frac{ \delta^2 S_{YY} }{ \delta \Pi(\theta_c) \delta \Pi(\theta_b)  } [\Pi ]  & = & \sum_{a \vert tranche}   \delta \theta \left \{  \frac{\Pi_s}{(\Pi^2  - \Pi_s\Pi)} \frac{(\delta \Pi)^2}{ \delta \Pi(\theta_c) \delta \Pi(\theta_b)  } \right . \\
		&  &  +  \left . \left (  \frac{\Delta}{2\pi} \star \left ( \frac{1 }{(\Pi_s -\Pi) }  \frac{\delta \Pi}{\delta \Pi(\theta_b)}  \right )   \right ) \frac{\delta \Pi}{\delta \Pi(\theta_c)} \delta \theta+  \left (  \frac{\Delta}{2\pi} \star \left ( \frac{1}{( \Pi_s -  \Pi) } \frac{\delta \Pi}{\delta \Pi(\theta_c)} \right) \right )\frac{\delta \Pi}{\delta \Pi(\theta_b)} \delta \theta  \right . \\
		& & + \left .  \left (  \frac{\Delta}{2\pi} \star \left ( \frac{\Pi }{\Pi_s\Pi - \Pi_s^2 }\left (  \frac{\Delta}{2\pi} \star\frac{\delta \Pi}{\delta \Pi(\theta_c)} \right )\right ) \right ) \frac{\delta \Pi}{\delta \Pi(\theta_b)} (\delta \theta)^2  \right \}(\theta_a),	
	\end{eqnarray}
	
	Avec 
	
	\begin{eqnarray}
			\left  ( \frac{\Delta}{2\pi} \star \frac{\delta \Pi}{\delta \Pi(\theta_b) }  \right ) & = & 	\frac{\Delta ( \cdot - \theta_b )}{2\pi}
		\end{eqnarray}
	
	\begin{eqnarray}
		\frac{ \delta^2 S_{YY} }{ \delta \Pi(\theta_c) \delta \Pi(\theta_b)  } [\Pi ]  & = & \sum_{a \vert tranche}   \delta \theta \left \{  \frac{\Pi_s}{(\Pi^2  - \Pi_s\Pi)} \frac{(\delta \Pi)^2}{ \delta \Pi(\theta_c) \delta \Pi(\theta_b)  } \right . \\
		&  &  +  \left . \frac{\Pi }{(\Pi_s\Pi -\Pi^2) } \left ( \frac{\Delta ( \cdot - \theta_c )}{2\pi}   \right ) \frac{\delta \Pi}{\delta \Pi(\theta_b)} \delta \theta+ \frac{\Pi_s }{( \Pi_s^2 - \Pi_s \Pi) } \left (  \frac{\Delta ( \cdot - \theta_b )}{2\pi} \right ) \frac{\delta \Pi}{\delta \Pi(\theta_c)} \delta \theta \right . \\
		& & + \left .  \frac{\Pi }{\Pi_s\Pi - \Pi_s^2 }\left (  \frac{\Delta ( \cdot - \theta_c )}{2\pi} \right )\left (  \frac{\Delta ( \cdot - \theta_b )}{2\pi} \right )( \delta \theta)^2  \right \}(\theta_a),	
	\end{eqnarray}
	
	
		


		
	%Or $2\pi \Pi_s(\theta_a)  = L + \sum_{c \vert tranche } \Delta( \theta_a  - \theta_c ) \Pi(\theta_c)  $ donc $ 2 \pi \frac{\delta \Pi_s(\theta_a)  }{ \delta \Pi(\theta_b) }  = \sum_{c \vert tranche } \Delta ( \theta_a - \theta_c )   \frac{\delta \Pi(\theta_c)  }{ \delta \Pi(\theta_b) }$ et de plus $\frac{\delta \Pi(\theta_a)  }{ \delta \Pi(\theta_b) } = \delta ( a - b ) $ donc  $ 2 \pi \frac{\delta \Pi_s(\theta_a)  }{ \delta \Pi(\theta_b) }  = \Delta ( \theta_a - \theta_b )   $ et 
%	\begin{eqnarray}
%		\frac{ \delta S_{YY} }{ \delta \Pi(\theta_b)  } [\Pi ]  & = &   \ln \left ( \frac{ \Pi_s(\theta_b) }{ \Pi (\theta_b) } - 1 \right )  \delta \theta  -  \sum_{a\vert tranche}  \left \{  \frac{ \Delta( \theta_a - \theta_b )}{2\pi} \left ( \ln \left ( 1 - \frac{ \Pi(\theta_a)}{ \Pi_s(\theta_a) }\right ) \right ) \right \}  \delta \theta  	
%	\end{eqnarray}
%	
	%Je vais noter le produit de convolution discret $( f \star g )(x) = \sum_{x'\vert tranche } f( x - x') g(x') $ 
	
	%En remarquant que $\Delta$ est symetrique il vient que : 
	
	\begin{eqnarray}
		\frac{ \delta S_{YY} }{ \delta \Pi(\theta_a)  } [\Pi ]  & = &    \left \{ \ln \left ( \frac{ \Pi_s }{ \Pi  } - 1 \right )   -  \frac{ \Delta}{2\pi}  \star \left ( \ln \left ( 1 - \frac{ \Pi}{ \Pi_s }\right ) \right ) \delta \theta \right \} (\theta_a) \delta \theta , \\
		\frac{ \delta^2 S_{YY} }{ \delta \Pi(\theta_c) \delta \Pi(\theta_b)  } [\Pi ]  & = &   \left (  \frac{\Pi_s}{(\Pi^2  - \Pi_s\Pi)}    \right )(\theta_b) \delta( \theta_b - \theta_c)  \delta \theta  \\
		&  &  +   \frac{ \Delta ( \theta_b - \theta_c)}{ 2\pi}  \frac{1 }{(\Pi_s(\theta_b) -\Pi(\theta_b)) } (\delta \theta)^2 +  \frac{\Delta ( \theta_c - \theta_b )}{2\pi}\frac{1 }{( \Pi_s(\theta_c)  -   \Pi(\theta_c) ) }(\delta \theta)^2  \\
		& & + \sum_{a \vert tranche}   \delta \theta \left \{  \frac{\Pi }{\Pi_s\Pi - \Pi_s^2 }\left (  \frac{\Delta ( \cdot - \theta_c )}{2\pi} \right )\left (  \frac{\Delta ( \cdot - \theta_b )}{2\pi} \right )( \delta \theta)^2  \right \}(\theta_a) ,	 	
	\end{eqnarray}
	
	

		
	Or $\Delta$ symetrique 
		
	\begin{eqnarray}
		\frac{ \delta^2 S_{YY} }{ \delta \Pi(\theta_c) \delta \Pi(\theta_b)  } [\Pi ]  & = &   \left (  \frac{\Pi_s}{(\Pi^2  - \Pi_s\Pi)}    \right )(\theta_b) \delta( \theta_b - \theta_c)  \delta \theta  \\
		&  &  +   \frac{ \Delta ( \theta_b - \theta_c)}{ 2\pi} \left (  \frac{1 }{(\Pi_s(\theta_b) -\Pi(\theta_b)) }  +  \frac{1 }{( \Pi_s(\theta_c)  -   \Pi(\theta_c) ) } \right )(\delta \theta)^2  \\
		& & + \sum_{a \vert tranche}   \left \{  \frac{\Pi }{\Pi_s\Pi - \Pi_s^2 }\left (  \frac{\Delta ( \cdot - \theta_c )}{2\pi} \right )\left (  \frac{\Delta ( \cdot - \theta_b )}{2\pi} \right )  \right \}(\theta_a)( \delta \theta)^3 ,		
	\end{eqnarray}
